Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, amp=True, amp_level='O2', arch='transformer_wmt_en_de_base_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='./data/wmt14_en_de_joined_dict', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=1000, lr=[0.0006], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=1, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='./checkpoints.base.1.1.amp/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=1, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[8], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| ./data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 61364224
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
| model transformer_wmt_en_de_base_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 1 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Transformer | epoch 0 | step 125 |avg loss 14.150 |avg tokens 36387.384 |tokens/s 54610.646 |walltime 83.288 |
Transformer | epoch 0 | step 250 |avg loss 12.305 |avg tokens 35784.168 |tokens/s 68812.052 |walltime 148.292 |
Transformer | epoch 0 | step 375 |avg loss 11.430 |avg tokens 36566.848 |tokens/s 69864.322 |walltime 213.716 |
Transformer | epoch 0 | step 500 |avg loss 11.140 |avg tokens 35885.824 |tokens/s 69038.908 |walltime 278.690 |
Transformer | epoch 0 | step 625 |avg loss 10.725 |avg tokens 36389.816 |tokens/s 70683.343 |walltime 343.044 |
Transformer | epoch 0 | step 750 |avg loss 10.238 |avg tokens 36314.360 |tokens/s 69388.682 |walltime 408.462 |
Transformer | epoch 0 | step 875 |avg loss 9.796 |avg tokens 36154.544 |tokens/s 68156.450 |walltime 474.770 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Transformer | epoch 0 | step 1000 |avg loss 9.453 |avg tokens 36298.880 |tokens/s 69825.561 |walltime 539.752 |
Transformer | epoch 0 | step 1125 |avg loss 9.190 |avg tokens 36279.136 |tokens/s 70614.827 |walltime 603.972 |
Transformer | epoch 0 | step 1250 |avg loss 8.888 |avg tokens 36206.944 |tokens/s 70129.602 |walltime 668.508 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Transformer | epoch 0 | step 1375 |avg loss 8.620 |avg tokens 36041.632 |tokens/s 66266.690 |walltime 736.494 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Transformer | epoch 0 | step 1500 |avg loss 8.378 |avg tokens 36101.248 |tokens/s 70311.654 |walltime 800.674 |
Transformer | epoch 0 | step 1625 |avg loss 8.087 |avg tokens 35984.048 |tokens/s 67086.027 |walltime 867.723 |
Transformer | epoch 0 | step 1750 |avg loss 7.792 |avg tokens 36021.528 |tokens/s 68224.243 |walltime 933.721 |
Transformer | epoch 0 | step 1875 |avg loss 7.567 |avg tokens 36152.552 |tokens/s 67898.876 |walltime 1000.277 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Transformer | epoch 0 | step 2000 |avg loss 7.302 |avg tokens 36225.408 |tokens/s 69871.841 |walltime 1065.084 |
Transformer | epoch 0 | step 2125 |avg loss 7.008 |avg tokens 36335.096 |tokens/s 70396.757 |walltime 1129.602 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Transformer | epoch 0 | step 2250 |avg loss 6.783 |avg tokens 35841.072 |tokens/s 70005.639 |walltime 1193.599 |
Transformer | epoch 0 | step 2375 |avg loss 6.605 |avg tokens 36231.720 |tokens/s 70162.487 |walltime 1258.149 |
Transformer | epoch 0 | step 2500 |avg loss 6.474 |avg tokens 36170.304 |tokens/s 69501.170 |walltime 1323.202 |
Transformer | epoch 0 | step 2625 |avg loss 6.371 |avg tokens 36115.632 |tokens/s 69271.479 |walltime 1388.373 |
Transformer | epoch 0 | step 2750 |avg loss 6.274 |avg tokens 36076.352 |tokens/s 70530.841 |walltime 1452.310 |
Transformer | epoch 0 | step 2875 |avg loss 6.169 |avg tokens 36143.536 |tokens/s 65888.418 |walltime 1520.879 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Transformer | epoch 0 | step 3000 |avg loss 6.054 |avg tokens 35936.920 |tokens/s 68584.821 |walltime 1586.377 |
Transformer | epoch 0 | step 3125 |avg loss 5.953 |avg tokens 36269.896 |tokens/s 69974.578 |walltime 1651.168 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Transformer | epoch 0 | step 3250 |avg loss 5.920 |avg tokens 35952.632 |tokens/s 69638.593 |walltime 1715.702 |
Transformer | epoch 0 | step 3375 |avg loss 5.861 |avg tokens 36041.032 |tokens/s 69727.305 |walltime 1780.313 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Transformer | epoch 0 | step 3500 |avg loss 5.803 |avg tokens 36426.536 |tokens/s 69694.207 |walltime 1845.646 |
Transformer | epoch 0 | step 3625 |avg loss 5.844 |avg tokens 35884.120 |tokens/s 66621.963 |walltime 1912.973 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Transformer | epoch 0 | step 3750 |avg loss 5.757 |avg tokens 36283.616 |tokens/s 70283.931 |walltime 1977.504 |
Transformer | epoch 0 | step 3875 |avg loss 5.694 |avg tokens 36152.896 |tokens/s 68830.564 |walltime 2043.159 |
Epoch time: 2058.7151362895966
Transformer | epoch 0 | step 3935 |avg loss 5.570 |avg tokens 36255.650 |tokens/s 69207.923 |walltime 2074.591 |
Validation loss on subset valid: 5.207270614295645
| done training in 2082.6 seconds
Transformer | epoch 0 | step RUN |avg loss 5.207 |walltime 2098.582 |
